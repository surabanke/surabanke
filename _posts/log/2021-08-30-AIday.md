---
title : "tesla ai day"

date : 2021-08-30

categories : log
---

내용은 재밌는데 많은 정보를 짧은 시간에 전달하려니 발표자들 말이 빠르다;
발표의 전체적인 흐름은 우리는 자율주행하는데 카메라만 쓸거다.
그렇게 하려고 어떻게 했는지.
카메라에서 어떻게 이미지를 받아들이고 처리하고, 구해진 정보들로 로드 플래닝을 어떻게 하는지.
그리고 수많은 도로 데이터의 라벨링을 어떤식으로 하는지 쭉 설명한다.


먼저 카메라에서 2D 이미지 정보를 얻어 3D vector space로 변환하는 부분을 설명하는데 차량 곳곳에 달린 카메라의 이미지를 인풋으로 하고 딥러닝 네트워크의 stem 부분은 resnet을 사용하여
regNet이라는 네트워크로 여러 이미지 output size를 달리해서 추출한다.(이때 카메라 갯수대로 네트워크를 사용함)

그러면 이미지는 저해상도 high dimension channel, 고해상도 low dimension channel이 다양하게 구해져서
전체 이미지의 위치 특성과 객체 모양의 특징점들을 모두 알 수 있다는 이점을 가진다.

아래 그림처럼 residual block에서 나오는 output이 다 다르다.


<img src = "/surabanke/assets/images/20210830_img1.png" width = "400">


BiFPN module에서는 regNet에서 나온 서로 다른 feature를 fusion하여 정보를 공유한다.

BiFPN은 각 이미지에 가중치를 더 주어서 그 특징이 더 도드라져보이게 한다는데 보면 각 layer에서 나온 feature를 서로 섞는 것같다.


<img src = "/surabanke/assets/images/20210830_img2.png" width = "400">


이 Multi scale feature들을 가지고
output공간이  위치를 인코딩한 map이라고 보고 이것의 픽셀을 쿼리, 차량에 달린 카메라에서 얻을 수 있는 2d이미지는 키,
그 쿼리와 키의 유사도가 곱해질 인코더의 output을 value로 해서 이미지와 map의 유사도를 계산하고  multi-head attention을 사용해서 3D 벡터 공간을 표현 한다.
그러면 시간순 대로 벡터공간 데이터를 얻을 수 있는데 이걸 push&pop 을 이용해서 queue에 쌓는다.(아래 그림의 노란 동그라미 부분)

<img src = "/surabanke/assets/images/20210830_img3.png" width = "500">

일정시간과 일정 거리마다 kinematics정보, multi feature cam 정보, position encoding 정보(attention에서 쓰이는 위치 정보)를 queue에 push한다.
그리고 spatial RNN이 video feature로 나타낸다. RNN의 hidden state에서 추출한 정보들은 아래그림과 같이 실시간으로 도로 정보를 그려낸다.


<img src = "/surabanke/assets/images/20210830_img4.png" width = "600">

이렇게 구한 vector space정보를 플래닝을 하는데 사용한다.
플래닝은 일반 휴리스틱기법으로 하면 너무 많은 시도끝에 목적지에 닿기때문에 monte carlo search기법(MCTS)을 사용한다고 한다.
이렇게 하면 어느 상태에서 최대의 value 값을 가지는 다음 state를 선택하며 최종 목적지에 닿을 수 있는 방법을 찾을 수 있다.
이때 발표 슬라이드에 어떤 장애물에 경로가 차단되어 목적지까지 가지 못하는 경우 뿐만아니라 탑승자의 불편함이 높은 (예를들면 경로 급브레이크를 밟고 옆 차선으로 끼어들어야하는 경로)경우도 올바른 플랜에 들어가지 못한다고한다.
당연히 그래야겠지만. 실무는 여러가지를 고려해야 하는군.
