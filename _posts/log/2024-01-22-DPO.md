---
title : "LLM Preference Learning에 대해"

date : 2024-01-22

categories : log
---

### Preference learning이란
보통 대형언어모델(LLM)에서 인간의 선호도를 따라 자연스러운 문장생성을 하도록 하는데 사용하는 기술을 말한다.

GPT-3이라고 불리는 Instruct GPT의 RLHF(Reinforcement Learning Human Feedback)가 잘 알려져있다.
여기서는 3가지 단계로 GPT를 튜닝한다.

첫번째로 휴먼 라벨러가 직접 선호된 응답을 작성한 데이터 셋(dataset of interest)으로 학습 한다.
이 모델을 SFT(supervised fine tuning)모델이라고 부른다.

두번째는 리워드 모델(Reward Model,RM)이다.
새로운 prompt에 대한 응답이 들어와도 리워드값을 예측하는 모델(RM) 자체를 학습하는 것이다.
리워드 모델 역시 사람이 직접 선호도를 작성한 데이터셋으로 학습한다.

데이터셋은 {σ1,σ2,µ}로 구성되어있다.
σ1,σ2는 프롬프트에 대한 응답문장이고 µ는 이 두개의 응답에 대한 사용자의 선호도 분포가 된다.
만약 사람이 두 응답을 동등하게 선호한다고 한다면 µ는 균일하다.

<img src = "/surabanke/assets/images/BT_Model.png" width = "400">

위 식은 Bradley-Terry model이라고 하며 preference modeling할때 많이 사용한다한다.
각 term은 응답에 대한 보상을 지수화한 것이며 이 값이 클수록 각 응답에 대한 선호도가 큰 것이다.
분자는 선호되는 답변의 지수 보상이고 분모는 두 답변 보상의 합으로 이 비율을 통해 어느 것이 더 선호되는 답변인지 알 수 있다.

그림에서는 1번응답이 2번응답보다 선호될 확률을 구하고있다.

<img src = "/surabanke/assets/images/RMloss.png" width = "400">

위는 RM의 loss식으로 선호도가 높은 응답과 선호도가 낮은 응답의 차이가 클수록 loss가 감소한다.

세번째는 PPO로 LLM(SFT model)을 fine tuning한다.

<img src = "/surabanke/assets/images/step3.png" width = "400">

새로운 prompt가 들어오고 LLM은 답변을 생성한다. 이때 RM의 value prediction으로 구해진 reward는 policy학습에 사용된다.
PPO의 objective function은 다음과 같다.

<img src = "/surabanke/assets/images/objectiveRLHF.png" width = "600">

π_r은 업데이트 된 policy
π_s는 기존 SFT policy
뒤에 term은 특정 NLP task에서 성능 저하를 관측 후 추가되었다고한다.
(너무 사전학습 분포와 다른 fine tuning데이터셋을 사용해서 그러는 것같아서 pre-training(Ep)데이터 셋을 샘플링한 x로 구한 값을 추가)

PPO는 학습 안정성을 고려하여 이전 policy와 업데이트된 policy사이 변화된 값이 너무 크게 변화하지 않도록 KL div형식으로 objective function을 정의한다.
아래 식도 이와같이 기존 SFT policy와 업데이트 된 policy사이에 KL div를 구하여 패널티를 준다.


RLHF는 Reward Model과 RLAgent학습에 상당한 계산비용이 발생한다.

DPO(Direct Preference Optimization)는 PPO-based RLHF의 두 절차를 하나로 결합한다.
RM을 없애고 binary cross entropy objective를 이용해 인간의 선호도에 기반한 policy를 직접 생성한다.

<img src = "/surabanke/assets/images/objectiveRLHF2.png" width = "400">

처음에는 위의 RLHF objective function을 그대로 가져온다.
수식은 아래와 같이 변형해준다.

<img src = "/surabanke/assets/images/change_shape.jpeg" width = "600">


마지막 형광줄이 그어져있는 식은 결국 아래식과 같다.
업데이트 이전의 policy와 이후 policy 차이에 따라 KL패널티를 주는 것이 RLHF와 같다.

<img src = "/surabanke/assets/images/DPO_1.png" width = "500">

즉 업데이트 이전의 policy가 최적이 될때가 목표가되기때문에 형광줄 식에서 분모값이 optimal solution이된다.

<img src = "/surabanke/assets/images/DPO_2.png" width = "700">

위 식은 아래와 같이 r(x,y)형태로 나타낼수있으며 여러 변형을 거쳐 DPO의 loss식으로 거듭난다.

<img src = "/surabanke/assets/images/DPO_3.png" width = "600">

(여기서 y_w가 y_l보다 선호되는 답변이다.)
DPO gradient의 첫번째 부분은 답변 l과 w의 estimated reward값을 계산한 것이며 잘못 예측되었을때 큰 weight를 가진다.
두번째 부분은 y_w의 확률은 증가하고 y_l의 확률은 감소하는 방향으로 모델을 학습시키기 위한 부분이다.
β는 gradient update강도를 조절하는 인자이다.
(위 수식에서 앞의 리워드 weight은 모델 update의 안정성을 위해 constraint 역할을 하고있다.
이를 사용하지 않고 likelihood만 가지고 학습한다면 아래 문장생성 예시처럼 성능이 급감소한다고한다.)

<img src = "/surabanke/assets/images/DPOerror.png" width = "500">


글을 마치며
DPO가 out-of-distribution에 강한지 살펴볼 필요가 있다.
여기서는 SFT 모델을 학습한 데이터셋과 비슷한 분포를 가진 preference 데이터셋을 사용한다고 가정한다.
그리고 논문은 6B모델에 대한 내용이었기에 큰 모델에 대해서도 PPO based RLHF와 성능을 검증한 결과가 나왔으면 한다.

______________________________________________
[OpenAI research](https://openai.com/research/instruction-following)

[DPO paper](https://arxiv.org/abs/2305.18290)
