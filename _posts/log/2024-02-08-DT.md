---
title : "Decision Transformer에 대해서"

date : 2024-02-08

categories : log
---

## Decision Transformer via Sequence Modeling
Decision Transformer frame offline RL to Sequence Modeling Problem


offline RL이기에 논문에서는 Behavior Cloning과 Conservative Q-learning 두 알고리즘과 비교한다.

value function 이나 policy gradients 계산 없이 그저 action을 output으로 한다.

강화학습의 구현하기 어렵고 해석하기 어려운 점을 쉽게 대체할 수 있고
트랜스포머는 이미 많은 문제들에서 효과가 검증된 architecture를 갖고있다.

casually masked transformer를 사용한다.

autoregressive한 model이다.(굳이 transformer가 아니더라도 autoregressive model이거나 lstm을 사용해서도 적용가능 할 것이다.)

<img src = "/surabanke/assets/images/lstm_DT.png" width = "300">

근데 이렇게하면 backprop할때 좀 복잡하다.
좋은 action이 50 step이전일수도있고 500step이전일수도있고
저거는 즉각적인 리워드만 얻는 거니까 저 경우에 output을 value function으로 대체하던지 해야한다.
이것은 credit assignment problem에 해당한다.

Decision Transformer에서는 이 모든것이 필요가 없다.
긴 시간동안의 credit assignment를 어텐션 메커니즘을 사용해서 1 step만에 해결가능하다.
어텐션은 어떤 sequance element든 다른 elements와 연결된다.
구조적으로 1 step만에 어느 action이 결정타였는지 비교해볼수있다.

<img src = "/surabanke/assets/images/DT.png" width = "600">

Decision Transformer 구조를 살펴보면 reward가 먼저 들어가고 그 reward를 얻기위한 action을 예측하는 순서로 진행된다.

즉, 내가 원하는 desired reward를 state와 함께 먼저 input으로 제공한 뒤 그 값을 얻기위한 action을 예측한다.

Decision Transformer는 많은 데이터가 있는 buffer에서 desired reward를 만족하는 action을 출력하는데 데이터는 논문에서는 DQN으로 뽑은 벤치마크 데이터를 사용했다고한다.(그저 랜덤이아니라 expert data라고 할수있겠다.) 그렇기에 supervised learning의 일종으로 봐도 무관하다.


이렇게 먼저 reward를 정해주고 그에 맞는 action을 구하는 방식으로 traditional RL이 시도하지 않았던 이유는

state → model → action의 기본 deepRL형태를 볼때 여기서 리워드가 인풋으로 더 들어가야하고

에이전트(모델)는 그냥 action을 해보는게 아니라 이 리워드를 갖기 위해서 어떤 action이 더 좋은지, 적합한지, 나쁜지, 더 복잡하게 생각해야한다.

DT는 Behaviour cloning과 비슷하지만 약간 다르다 BC는 expert를 모방하여 높은 리워드를 갖는 것에 집중한다면 DT는 이런 historical data가 주어졌을때 어떤 action을 가져야하는지에 집중한다.

### Context length problem

<img src = "/surabanke/assets/images/context_length_DT.png" width = "500">

DT에 들어가는 input history 데이터의 길이는 고정되어있다. 이를 context legnth라고한다.
만약 future reward를 갖는 주황색 노드 데이터가 처음 초록색 노드에서 했던 action에 영향을 받는다면
이것은 contet length를 넘어간다.
context가 정해져있어야하고 이를 넘는다면 고전적인 RL접근방식을 사용하여야한다는게 단점이다.
그러나 결과를 보면 Atari game에 한해서 다른 CQL, BC등 offline RL과 비교하였을때 더 좋은 결과를 보인다.

### Decision Pretrained Transformer(DPT)

few-shot prompting 또는 in-context learning이란 프롬프트로 모델에 적은 정보를 주고
미리 훈련된 모델이 정보를 통해 완성도가 높은 답변을 생성하도록 하는 프롬프팅 기법이다.

few-shot은 몇가지 질문과 답변 예시를 들어주는 것이고
in-context learning은 말그대로 주어진 정보안에서 학습성능을 좀 더 높이는 것을 말한다.
(zero shot, one shot, few shot이 이에 해당한다.)

이 논문에서는 위의 기법들을 사용하여 강화학습 설정에 적용된 연속적 의사결정에 적용된 문맥을 학습하고 이를 이해하는데 pretrained transformer를 사용하는 것이다.

Decision Pretrained Transformer(DPT)는 데이터셋이 주어졌을때 최적의 action을 예측하는 트랜스포머다.
DPT는 학습할 때 데이터셋에서의 상황만 주어지기 때문에 새로운 상황과 문제에 대처할 수 있을까싶지만 의외로 잘한다. DPT는 새로운 상황의 대처법으로 ‘exploration strategy’를 사용한다.

DPT는 offline online 어디서든 다양한 의사결정 문제에 일반화된다.

1. 이전에 학습하지 않았던 보상체계에서도
2. 간단한 (의사결정과정)MDP라면 새로운 목표, 규칙을 가지는 문제에서도 잘 작동한다.

**즉, DPT는 in-context learning를 사용했기에 매개변수 업데이트 없이 다양한 상황에 유연하게 적용될 수 있는 알고리즘이다.**

DPT는 예를들어 parametric bandit 문제에서 (이는 선형보상 구조를 사용한다.) 이런 사전 정보를 몰라도 선형 보상을 사용하는것과 비슷하게 작동한다. 즉 새로운 문제의 구조를 파악하고 해결책을 찾아내는 것이다.

<img src = "/surabanke/assets/images/DPT.png" width = "600">

주황색의 데이터 셋들을 D라고 하고 맥락 내 데이터셋이라고 한다.

delta는 시간에 따라 달라지는 trajectory데이터이며 여기서 transformer가 맥락적인 정보를 알아내어 액션을 구할 수있다.

그리고 D에서 s_query를 샘플링하고 문제에 대한 최적 Policy에서 a*query를 샘플링한다.
이 논문에서는 사용한 GPT-2 Transformer decoder model의 loss를 아래와 같이 정의했다.

<img src = "/surabanke/assets/images/DPTloss.png" width = "300">

loss식을 보면 학습할때 모델에 제공하는 데이터가 맥락 정보를 제공하는 in-context dataset말고도 s_query가 있다.

inference시에는 지금 action을 예측해야하는 state에 대해서만 제공할 것이다.
trajectory들을 offline datasets이라고 하지않고 in-context dataset이라고 명시한 데에는 학습 중 task의 흐름을 정보로 준다는 것이 들어있는 것같다.

2가지 아쉬운것은
pretrained 과정에서 다양한 task에 대해 사전학습을하여 action distribution이 다양화된다면 일반화 성능이 어느정도로 나오는지 보여줬으면 좋겠다. DPT가 새로운 task에도 성능이 잘 나온다는 걸 봤으면했다.

Decision Transformer보다 더 나은점이 결국에는 optimal action을 학습에 사용해서 regret을 줄였다는 것인데 그러기위해 최적화된 다른 RL에이전트를 써야한다는게 아쉽다.


아래 글들을 참고하였다.
_____________________________________________

[The old RL is Dead, Transformer will be the new RL](https://ai.plainenglish.io/reinforcement-learning-is-dead-long-live-the-transformer-228835689841)

[DT youtube review](https://www.youtube.com/watch?v=-buULmf7dec&list=LL&index=1)

[DT](https://arxiv.org/pdf/2106.01345.pdf)

[OnlineDT](https://arxiv.org/pdf/2202.05607.pdf)

[DPT](https://arxiv.org/pdf/2306.14892.pdf)
