---
title : "Dynamic Programming"

date : 2020-10-27

categories : rl
---

## Model Based Planning, Dynamic Programming

MDPë¥¼ ì•Œë•Œ ëª¨ë¸ì˜ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ì¢‹ì€ policyë¥¼ ì°¾ëŠ” ê²ƒì„ planningì´ë¼ê³  í•œë‹¤.
ë™ì  : ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ëŒ€ìƒì„
í”„ë¡œê·¸ë˜ë° : ì—¬ëŸ¬ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¡œ ë‚˜ëˆ„ì–´ planning

### Prediction ê³¼ Control

#### Prediction : Estimate the value function

Input : MDP <S,A,P,R,Î³> and policy ğ¿ or MRP <S,P,R,Î³>

Output : value function(state or action-state function)

ê¸°ì¡´ policyì˜ evaluationë§Œ í•œë‹¤.

#### Control : Optimise the value function and find well-made policy

Input: MDP <S,A,P,R,Î³>

Output : optimal value function and optimal policy

policy evaluate + improveê¹Œì§€ í•œë‹¤.


- Policy iteration

evaluateê³¼ imporveë¥¼ ë°˜ë³µí•˜ë©° ìµœì  policyë¥¼ ì°¾ëŠ”ë‹¤.
ì²˜ìŒ policyë¥¼ ì´ˆê¸°í™”í•˜ê³  í‰ê°€ë‹¨ê³„ì—ì„œ policyì— ëŒ€í•´ valueë¥¼ êµ¬í•œë‹¤.
policy imporveë‹¨ê³„ì—ì„  ìƒˆë¡œìš´ policyë¡œ ì—…ë°ì´íŠ¸ë¥¼ í•˜ëŠ”ë° ìƒˆë¡œìš´ policyëŠ” ì•ì—ì„œ êµ¬í•´ë†“ì€ value function greedyí•˜ê²Œ í•´ì¤€ê°’ì´ë‹¤.
value functionì€ bellman expectation equationìœ¼ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.

\[
ğ¿'(s) = argmax q(s,a)
\]

ì´ ì ˆì°¨ë¥¼ ë°˜ë³µ í•˜ë‹¤ë³´ë©´ optimal policyì— ìˆ˜ë ´í•˜ê²Œ ëœë‹¤.

- Value iteration

policy iterationì€ ë§¤ iterë§ˆë‹¤ policy evaluationì„ í•œë‹¤.
í•˜ì§€ë§Œ ì´ value iterationì€ ìµœì ì˜ policyê°€ ë§Œë“¤ì–´ë‚´ëŠ” ìµœì  valueë§Œì„ ì°¾ëŠ”ë‹¤.

value functionì€ bellman optimal euqationìœ¼ë¡œ í•œë‹¤.
