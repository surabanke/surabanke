---
title : "COMA"

date : 2021-04-16

categories : rl_paper
---

## Counterfactual Multi Agent Policy Gradient

#### DEC-POMDP (Decentralized Partially Observable Markov Decision Process)
#### Centralized training, Decentralized execution
#### 각 Agent 개별보상이 아니라 team reward -> credit assignment 문제가 발생




COMA는 credit assignment문제를 default action을 쓴 reward와 team reward의 차이를 비교하여 그것으로 PG를 구한다.

single agent DQN

<img src = "/surabanke/assets/images/d1.jpeg" width = "400">

single agent Actor Critic

<img src = "/surabanke/assets/images/d2.jpeg" width = "400">


First, COMA uses a centralised Critic all state information is available.

Second, COMA uses a counterfactual baseline
현재 Q값 - 다른 agent action고정시키고 단일에이전트만 행동을 바꿔보면서 adventage function계산

Third, COMA uses a critic representation that allows the counterfactual baseline to be computed efficiently. Critic에서 단한번의 single forward pass로 모든 agent에대한 Q-value를 구함



"Naive way to use this centralised critic with TD Actor-Critic policy gradient."

<img src = "/surabanke/assets/images/TDd3.png" width = "500">

TD error는 global reward만 고려하기 때문에 credir assignment문제를 해결할수 없음


<img src = "/surabanke/assets/images/d4.png" width = "300">


그래서 COMA는 counterfactual baseline을 사용하는데 이 아이디어는 difference rewards에서 영감을 받았다고한다.
위의 식이 diffrence reward이다 joint action으로 받은 rewrd에서 u-a 로 다른 에이전트 action을 고정하고 (user specified default action) c-a를 다를게 하면서 리워드 차이를 관찰한다.


<img src = "/surabanke/assets/images/COMAd5.png" width = "400">


4번 식은 팀관점 Q값에서 (다른 agent의 joint action을 고정한 채 지금 agent 입장에서 본 partially ovservable한 state에서 할 수 있는 action에 따른 value값을 모두 해본 값)을 빼서 adventage function을 구한것이다. 이 식에서 V(s)에 해당하는 값을 counterfactual baseline이라고 한다.
