---
title : "SAC"

date : 2021-06-13

categories : rl_paper
---

## Soft Actor-Critic
#### : Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor

Keywords: model free, off policy, stochstic RL, maximum entropy, actor-critic

abstract에서 이 논문의 핵심을 언급한다.
"In this framework, the actor aims to maximize expected reward while also maximizing entropy.
That is, to succeed at the task while acting as randomly as possible."
"actor는 expected retward를 최대로 하되, entropy도 최대로 해야한다. 즉, 가능한 랜덤하게 action을 취해보면서 주어진 과제도 잘 수행해야함."

SAC의 3가지 특징은 actor-critic architecture, off-policy formulation, entropy maximization 이다.

먼저 real world에 기존 model-free RL들은 사용하려면 data가 많이 필요하다는 것을 문제점으로 집는다. 복잡한 action이면 data는 더 필요하다.
sample efficiency를 더 어렵게 하는 것으로 on-policy RL들이 특히 더 그렇다. 과거policy로 얻은 data는 재사용이 불가능하기 때문이다.
즉, policy를 update할 때마다 새로운 data가 필요하다.
두번째로 하이퍼파라미터 조정의 어려움을 든다. (off policy + high action dimension에서 + nonlinear neural net)을 사용하는 DDPG는 안정성이 취약하고 hyperparameter에 민감하다.

SAC는 기존에 Reward를 maximize하는 objective function에 entropy maximization term을 추가한것으로 efficient하고 stable한 model free deep RL이며 연속 action task와 on-policy, off-policy 부분에서 SOTA를 달성했다고 주장한다.


#### Maximum entropy RL


<img src = "/surabanke/assets/images/maxumum_entropy_objective.PNG" width = "700">


위의 식은 현 state에서 열린탐색을 한다. => 최적에 가까운 여러 action을 찾는 policy -> 최악의 policy를 선택할 확률 minimize.
alpha는 탐색과 리워드 간 상대적 중요성을 나타내는 파라미터이다. alpha를 0으로하면 기존 RL의 objective function이다.
(논문에서 maximization entropy term을 넣어주면 빠른 습득이 가능하다고 써져있는데 어디서 ppo보다 같은 리워드를 받기까지 걸리는 step수가 더 적다는 글을 읽은적이 있다.)


SAC에서는 critic에서 Q를 학습하고 actor에서 objective function을 학습하는데 필요한 파라미터가 4개다.



<img src = "/surabanke/assets/images/SAC_algorithm.PNG" width = "700">



왜냐하면
SAC 이전에 soft policy iteration을 보면
soft state value function은


<img src = "/surabanke/assets/images/soft_state_value.PNG" width = "700">


로 기존의 state vlue function에 entropy term을 붙였다.
그리고 soft q function은


<img src = "/surabanke/assets/images/soft_q_value.PNG" width = "700">


q function을 정의하는데도 soft state value가 들어가게 된다. 따라서 critic에서 학습하는 soft q value에도 역시 soft state value가 필요해진다. 이런 순환구조를 막기 위해 soft state value main network에서 학습한 파라미터를 업데이트해서 target state value network에서 구한 state soft value를 soft q-function에 사용한다. (DQN의 target network 방식이랑 같은 효과를 볼 수 있음)


soft state value network의 loss식


<img src = "/surabanke/assets/images/soft_state_value_main.PNG" width = "700">


soft state value function loss의 gradient는


#### <img src = "/surabanke/assets/images/soft_state_value_gradient.PNG" width = "700">

-> 위의 Q와 π 값은 value network 파라미터와 관련이 없어서 no gradient임


soft q function network의 loss식


<img src = "/surabanke/assets/images/soft_q_value_loss.PNG" width = "700">


objective function은 policy π 와 q-value 사이의(softmax로 표현한다.) KL divergence를 구한다.


<img src = "/surabanke/assets/images/SAC_objective.PNG" width = "700">


<img src = "/surabanke/assets/images/SAC_objective1.PNG" width = "700">


쉽게 풀어쓰면 위에서 말했던 soft state value function을 maximize 해야 한다는 것과 KL-divergence objective function이 동치임을 알게 된다.

<img src = "/surabanke/assets/images/sac_same_meaning.PNG" width = "700">


이해 안가는 것
maximum entropy RL설명부분에서 이 objective function이 discount factor를 썻기때문에 infinite horizon문제도 풀수 있음
이라고 써져있는데 discount factor를 도입하니까 결국 미래보상은 수렴하고 합은 유한하기때문에 그러한 환경도 가능하다는 것일까?



(결과분석은 나중에)
