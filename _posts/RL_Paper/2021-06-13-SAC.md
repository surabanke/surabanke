---
title : "SAC"

date : 2021-06-13

categories : rl_paper
---

## Soft Actor-Critic
#### : Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor

#### Keywords: model free, off policy, stochstic RL, maximum entropy, actor-critic

abstract에서 이 논문의 핵심을 언급한다.
"In this framework, the actor aims to maximize expected reward while also maximizing entropy.
That is, to succeed at the task while acting as randomly as possible."
"actor는 expected retward를 최대로 하되, entropy도 최대로 해야한다. 즉, 가능한 랜덤하게 action을 취해보면서 주어진 과제도 잘 수행해야함."

SAC의 3가지 특징은
1. actor-critic architecture,
2. off-policy formulation,
3. entropy maximization 이다.

먼저 real world에 기존 model-free RL들은 사용하려면 data가 많이 필요하다는 것을 문제점으로 집는다. 복잡한 action이면 data는 더 필요하다.
sample efficiency를 더 어렵게 하는 것으로 on-policy RL들이 특히 더 그렇다. 과거policy로 얻은 data는 재사용이 불가능하기 때문이다.
즉, policy를 update할 때마다 새로운 data가 필요하다.
두번째로 하이퍼파라미터 조정의 어려움을 든다. (off policy + high action dimension에서 + nonlinear neural net)을 사용하는 DDPG는 안정성이 취약하고 hyperparameter에 민감하다.

SAC는 기존에 Reward를 maximize하는 objective function에 entropy maximization term을 추가한것으로 efficient하고 stable한 model free deep RL이며 연속 action task와 on-policy, off-policy 부분에서 SOTA를 달성했다고 주장한다.


#### Maximum entropy RL


<img src = "/surabanke/assets/images/maximum_entropy_objective.PNG" width = "700">


위의 식은 현 state에서 열린탐색을 한다. => 최적에 가까운 여러 action을 찾는 policy -> 최악의 policy를 선택할 확률 minimize.
alpha는 탐색과 리워드 간 상대적 중요성을 나타내는 파라미터이다. alpha를 0으로하면 기존 RL의 objective function이다.
(논문에서 maximization entropy term을 넣어주면 빠른 습득이 가능하다고 써져있는데 어디서 ppo보다 같은 리워드를 받기까지 걸리는 step수가 더 적다는 글을 읽은적이 있다.)

#### SAC

SAC에서는 critic에서 Q를 학습하고 actor에서 objective function을 학습하는데 필요한 파라미터가 4개다.



<img src = "/surabanke/assets/images/SAC_algorithm.PNG" width = "700">


여기서 이 알고리즘은 off-policy로 동작하기때문에 replay buffer를 볼 수있다.
그리고 q-function network 업데이트하는 부분을 자세히 보면 i ∈ {1,2}인데 q-function은 두개를 각각 따로 학습시키고 그 중 작은것을 골라서 value network와 policy network에 사용한다. (뒤의 식에 나오겠지만 policy와 value를 계산하는데 q-value가 필요함)
왜 q-fuction을 두개를 사용할까 -> 논문에서는 policy imporve 단계에서 positive bias한 value는 value-based methods에 성능저하를 일으킨다고 되어있다. 그래서 둘 중 작은 q값을 고르는 것 같다. 이러한 방법은 training speed를 빠르게 올려준다고한다.

이제 SAC에서 사용하는 value들과 policy 그리고 학습에 사용되는 식들을 보자.

일단 soft policy iteration에서 사용되었던 value들을 봐야한다.

soft state value function은


<img src = "/surabanke/assets/images/soft_state_value.PNG" width = "500">


로 기존의 state vlue function에 entropy term을 붙였다.
그리고 soft q function은


<img src = "/surabanke/assets/images/soft_q_value.PNG" width = "500">


q function을 정의하는데도 soft state value가 들어가게 된다. 따라서 critic에서 학습하는 soft q value에도 역시 soft state value가 필요해진다. 이런 순환구조를 막기 위해 soft state value main network에서 학습한 파라미터를 업데이트해서 target state value network에서 구한 state soft value를 soft q-function에 사용한다. (DQN의 target network 방식이랑 같은 효과를 볼 수 있음)


soft state value network의 loss식


<img src = "/surabanke/assets/images/soft_state_value_main.PNG" width = "700">


soft state value function loss의 gradient는


<img src = "/surabanke/assets/images/soft_state_value_gradient.PNG" width = "700">

-> 위의 Q와 π 값은 value network 파라미터와 관련이 없어서 no gradient임


soft q function network의 loss식


<img src = "/surabanke/assets/images/soft_q_value_loss.PNG" width = "600">


objective function은 policy π 와 q-value 사이의(softmax로 표현한다.) KL divergence를 구한다.


<img src = "/surabanke/assets/images/SAC_objective.PNG" width = "600">


<img src = "/surabanke/assets/images/SAC_objective1.PNG" width = "600">


쉽게 풀어쓰면 위에서 말했던 soft state value function을 maximize 해야 한다는 것과 KL-divergence objective function이 동치임을 알게 된다.

<img src = "/surabanke/assets/images/sac_same_meaning.PNG" width = "600">

objective function에 대한 나의 생각-
maximum entropy RL설명부분에서 이 objective function이 discount factor를 썻기때문에 infinite horizon문제도 풀수 있음
이라고 써져있는데 discount factor를 도입하니까 결국 미래보상은 수렴하고 합은 유한하기때문에 그러한 환경도 가능하다는 것같다.

맽랩 SAC 코드
<img src = "/surabanke/assets/images/SACmatlab0417.png" width = "700">
SAC는 model-free Stochastic policy며 continuous한 action space를 가진 real world에 사용하기 위해 만들어졌다고한다.
DDPG는 하이퍼파라미터 조정에 민감하고 학습에 어려움이 있어 SAC가 좋은 성과를 내는 경우도 있다고한다.
두 알고리즘은 exploration을 하는 방법에도 차이가 있다.
DDPG는 orsteinuhlenbeck random process 노이즈를 액션에 추가하며
SAC는 loss에 entropy를 추가한다.
연속 action space를 가지는 SAC 특성상 rlNumericSpec을 사용하여 범위 5~65로 action space를 정의하였다.
state는 LTEV2Vsim 의 CBR을 사용한다.
학습해야하는 네트워크는 critic 2개와 target critic 2개 actor하나로 구성되어있다.
매트랩 문서의 actor구조는 전부 unbounded되어있는데 그 이유는 에이전트에서 자동으로 스케일 조정을 하여 액션을 뽑아내기 때문이다.
나는 에이전트를 쓰지 않을거기 때문에 수동으로 tanh layer와 scale layer를 두어서 bound action을 만들고 actor로 액션을 구할것이다.
actor의 output은 각각 mean과 std value로 그 값을 토대로 가우시안 분포로 만든 후 랜덤으로 액션을 고르며 exploration한다.
또 entropy는 rl.option.EntropyWeightOptions를 사용해서 정의한다.
