---
title : "PPO"

date : 2020-12-10

categories : rl_paper
---


## Proximal Policy Optimization

trpo에서 언급했던 이전 policy gradient의 문제점은
1, policy를 계속 업데이트하면 과거 policy를 사용하지 못함.
2, parameter 의 작은 변화는 policy에서는 큰 변화이다. -> policy가 좋은 쪽으로 적당하게 변하는 step size를 찾아야 함.

즉, 지금 있는 데이터로 가능 한 큰 step으로 update 하고 싶지만 성능을 떨어트리고 싶지 않다.
=> 얼만큼 update해야 안전한가 를 TRPO에서는 lower bound와 penalty를 주는 constraint optimization으로 해결한다. -> second order
이것을 ppo는 first order로 더 practical하고 간단하게 해결한다.
논문 앞부분 에서는 PPO의 목적을 이렇게 설명한다.
"This paper seeks to improve the current state of affairs by introducing an algorithm that attains the data efficiency and reliable performance of TRPO, while using only first-order optimization."

policy gradient 형태는 

<img src = "/surabanke/assets/images/policy_gradient.PNG" width = "300">

이것을 현재 state 그리고 action 그 다음 state 순서의 trajectory를 따라서 전개 해보면

<img src = "/surabanke/assets/images/policy_gradient_1.PNG" width = "600">

<img src = "/surabanke/assets/images/policy_gradient_2.PNG" width = "600">

<img src = "/surabanke/assets/images/policy_gradient_3.PNG" width = "600">

처럼 되어서 θ를 update한다. 이때 update하는 부분을 크게 하고 싶으나 이전 policy와 지금 policy의 변화를 너무 크게 하고 싶진않으므로 kl divergence를 constraint로 주는 것이 trpo였다.
trpo형태로 만들면..

<img src = "/surabanke/assets/images/policy_gradient_4.PNG" width = "600">

PPO형식으로 만들려면 clipping 으로 제약조건을 없애준다.

<img src = "/surabanke/assets/images/policy_gradient_5.PNG" width = "400">

<img src = "/surabanke/assets/images/policy_gradient_6.PNG" width = "400">

<img src = "/surabanke/assets/images/policy_gradient_7.PNG" width = "500">

이 clipping 을 자세히 보면 A >= 0 일때 r*At가 (1+ε)At 보다 크면 min을 사용해 (1+ε)At를 고르게 된다.
즉 더 크면 clipping으로 자른다. A < 0 일때는 (1-ε)At보다 작으면 (1-ε)At를 고르도록 한다.

이 때 바로 위의 그림을 보면 L_clip에서 일반 At가 아니라 GAE를 사용한다고 되어있다.


### 이 GAE란 무엇일까?

Generalized Adventage Estimation

TD(λ)를 1- step 부터 ∞-step 까지 전개해보면

<img src = "/surabanke/assets/images/TD_lamda_nstep.PNG" width = "700">

그림에 써있는 것처럼 이 TD(λ)에 EMA를 적용하여 bias-variance trade-off 를 해준다.

<img src = "/surabanke/assets/images/TD_lambda.PNG" width = "200">

λ = 0.1 이라고 가정한다면

(0.9 * At) + (0.9 * 0.1 * At) + (0.9 * 0.1^2 * At).....

첫번째 td 1-step이 weight가 가장큰 것을 볼 수 있다. 전부 weighted sum 해주면 weight의 총합은 1이 된다.


EMA를 적용한 TD(λ)를 전개하면

<img src = "/surabanke/assets/images/GAE_1.PNG" width = "700">

이것을 다르게 전개하여 완성된 마지막 수식을 보면..

<img src = "/surabanke/assets/images/GAE_2.PNG" width = "200">

가 되고 이것을 GAE 라고 한다.

이것을 실용적이게 바꿔야한다. 지금의 GAE는 무한까지 계산하고 있다. 이것을 t까지로 바꾸어서

<img src = "/surabanke/assets/images/GAE_3.PNG" width = "200">

실제 논문에서는 위의 수식처럼 사용한다.
GAE는 gamma와 lamda를 사용하여 bias variance trade-off문제를 해결한다.

GAE..수식으로는 알겠는데 직관적으로 이해가 가지 않는다
