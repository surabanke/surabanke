# Front Matter
---
title : "PPO"

date : 2020-12-08

categories : rl_paper
---

## Proximal Policy Optimization

지금 있는 데이터로 가능 한 큰 step으로 update 하고 싶지만 성능을 떨어트리고 싶지 않음
=> 얼만큼 update해야 안전한가 를 TRPO에서는 lower bound와 penalty를 주는 constraint optimization으로 해결한다. -> second order
이것을 ppo는 first order로 더 practical하고 간단하게 해결한다.
논문 앞부분 에서는 PPO의 목적을 이렇게 설명한다.
"This paper seeks to improve the current state of affairs by introducing an algorithm that attains the data efficiency and reliable performance of TRPO, while using only first-order optimization."
 

먼저 기존에 알고 있던 policy gradient는 이렇다. 이것이 (1)식

<img src = "/surabanke/assets/images/vanila_policy_gradient.png" width = "400">

이것을 적분하면 이렇게 된다.

<img src = "/surabanke/assets/images/integral_pg.png" width = "400">

이 L_pg는 policy gradient의 Loss이다.
딥러닝에서 Loss(cost)function을 신경망 파라미터(여기서는 theta)에 대해 편미분하면 그것을 gradient라고 하여 sgd로 업데이트할때 사용하는데 거기서 가져온 이름인 거같다.(찾아봐야함, 왜 Loss라고 하는지 모르겠다.)

<img src = "/surabanke/assets/images/trpo_pg.png" width = "400">

위의 식은 trpo논문에서 나왔던 constraint optimization인데 조건으로 달은 KL-divergence빼고 maximize식을 미분하면 (1)식이 나온다.

<img src = "/surabanke/assets/images/trpo_pg.png" width = "400">
