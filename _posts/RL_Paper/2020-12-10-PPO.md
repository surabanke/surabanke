# Front Matter
---
title : "PPO"

date : 2020-12-08

categories : rl_paper
---

## Proximal Policy Optimization

지금 있는 데이터로 가능 한 큰 step으로 update 하고 싶지만 성능을 떨어트리고 싶지 않음
=> 얼만큼 update해야 안전한가 를 TRPO에서는 lower bound와 penalty를 주는 constraint optimization으로 해결한다. -> second order
이것을 ppo는 first order로 더 practical하고 간단하게 해결한다.
논문 앞부분 에서는 PPO의 목적을 이렇게 설명한다.
"This paper seeks to improve the current state of affairs by introducing an algorithm that attains the data efficiency and reliable performance of TRPO, while using only first-order optimization."
 

<<<<<<< HEAD
policy gradient 형태는 

<img src = "/surabanke/assets/images/policy_gradient.PNG" width = "300">

이것을 현재 state 그리고 action 그 다음 state 순서의 trajectory를 따라서 전개 해보면

<img src = "/surabanke/assets/images/policy_gradient_1.PNG" width = "600">

<img src = "/surabanke/assets/images/policy_gradient_2.PNG" width = "600">

<img src = "/surabanke/assets/images/policy_gradient_3.PNG" width = "600">

처럼 되어서 θ를 update한다. 이때 update하는 부분을 크게 하고 싶으나 이전 policy와 지금 policy의 변화를 너무 크게 하고 싶진않으므로 kl divergence를 constraint로 주는 것이 trpo였다.
trpo형태로 만들면..

<img src = "/surabanke/assets/images/policy_gradient_4.PNG" width = "600">

PPO형식으로 만들려면 clipping 으로 제약조건을 없애준다.

<img src = "/surabanke/assets/images/policy_gradient_5.PNG" width = "400">

<img src = "/surabanke/assets/images/policy_gradient_6.PNG" width = "400">

<img src = "/surabanke/assets/images/policy_gradient_7.PNG" width = "500">

이 clipping 을 자세히 보면 A >= 0 일때 r*At가 (1+ε)At 보다 크면 min을 사용해 (1+ε)At를 고르게 된다.
즉 더 크면 clipping으로 자른다. A < 0 일때는 (1-ε)At보다 작으면 (1-ε)At를 고르도록 한다.

이 때 바로 위의 그림을 보면 L_clip에서 일반 At가 아니라 GAE를 사용한다고 되어있다.


### 이 GAE란 무엇일까?

Generalized Adventage Estimation

TD(λ)를 1- step 부터 ∞-step 까지 전개해보면

<img src = "/surabanke/assets/images/TD_lamda_nstep.PNG" width = "700">

그림에 써있는 것처럼 이 TD(λ)에 EMA를 적용하여 bias-variance trade-off 를 해준다.

<img src = "/surabanke/assets/images/TD_lambda.PNG" width = "200">

λ = 0.1 이라고 가정한다면

(0.9 * At) + (0.9 * 0.1 * At) + (0.9 * 0.1^2 * At).....

첫번째 td 1-step이 weight가 가장큰 것을 볼 수 있다. 전부 weighted sum 해주면 weight의 총합은 1이 된다.


EMA를 적용한 TD(λ)를 전개하면

<img src = "/surabanke/assets/images/GAE_1.PNG" width = "700">
=======
먼저 기존에 알고 있던 policy gradient는 이렇다. 이것이 (1)식
>>>>>>> bd52edc403904d62f2c9105a81333b88b304a8a2

<img src = "/surabanke/assets/images/vanila_policy_gradient.png" width = "400">

이것을 적분하면 이렇게 된다.

<img src = "/surabanke/assets/images/integral_pg.png" width = "400">

이 L_pg는 policy gradient의 Loss이다.
딥러닝에서 Loss(cost)function을 신경망 파라미터(여기서는 theta)에 대해 편미분하면 그것을 gradient라고 하여 sgd로 업데이트할때 사용하는데 거기서 가져온 이름인 거같다.(찾아봐야함, 왜 Loss라고 하는지 모르겠다.)

<img src = "/surabanke/assets/images/trpo_pg.png" width = "400">

위의 식은 trpo논문에서 나왔던 constraint optimization인데 조건으로 달은 KL-divergence빼고 maximize식을 미분하면 (1)식이 나온다.

<img src = "/surabanke/assets/images/trpo_pg.png" width = "400">
