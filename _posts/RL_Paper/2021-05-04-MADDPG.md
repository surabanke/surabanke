---
title : "MADDPG"

date : 2021-05-04

categories : rl_paper
---

## Multi-Agent Actor-Critic for mixed Cooperative-Competitive Environments


COMA에 이어서 두번째로 읽은 CTDE논문이다.

COMA논문이 credit assignment문제에대해 중점을 뒀다면
MADDPG논문은 1.CTDE, 2.policy approximation of other agents, 3.policy ensemble기법을 이용하여 다양한 multi agent task를 하였을때 기존의 single agent와 차이를 분석한다.

####Introduction에서는 tradition RL이 multi agent문제에 적합하지 않다고하며 예시로 multi agent환경은 학습하면서 각 policy 가 달라지기때문에 행동하는 방식도 다르고 non-stationary한 envionment를 만든다고 한다.
그리고 이것은 DQN에서 사용하는 naive한 experience replay 기법을 쓰지 못하게 한다.   
PG기법들 또한 multi agent문제에서 매우 큰 variance가 나타나기 때문에 적합하지 않다.

그러므로 이 논문에서는 범용적인 멀티에이전트 기법을 제안하며 MADDPG는
1) execution할때 각자의 local observation을 정보로 사용해도 되며
2) 특정 모델 structure를 가정하지 않으며 agents간의 communication method를 따로 지정하지 않고
3) cooperative & competitive 한것을 둘다 요구하는 mixed interections환경에서 사용가능하다고 한다.

####Related works
CTDE를 적용하지 않은 이전의 멀티에이전트 기법인 independant learning agents들은 (IQL,IAC) 앞서 말했던 이슈인 non stationary한 env문제를 해결하기 위하여 다른 에이전트의 policy parameter를 Q function에 넣어보거나 replay buffer에 반복 인덱스를 명시하거나 importance sampling을 하여 trajectory data가 사용되는 확률분포를 다르게 하는 등의 시도를 했다.

####Methods
논문에서 3가지 제약을 말한 후 기법을 설명하는데 첫번째는 학습된 policy는 local observation만 사용한다는 것, 2번째는 특정한 환경 모델을 사용하지 않았다는 것, 3번째는 agent끼리 물리적 상호작용만 하는 환경에서 학습한거고 차별화된 의사소통 방법은 사용하지 않았다는 것이다.

앞서 말했듯이 MADDPG는 Centralized Training Decentralized Execution으로 구조는 논문의 Fugure 1 을 참고한다.

<img src = "/surabanke/assets/images/CTDEMADDPG.png" width = "400">

각agent는 observation과 action pair를 모든 agent의 critic에 전달한다.
그러므로 각 agent는 centralized된 환경 정보를 얻을 수 있다. 이렇게되면 각 agent가 택한 action을 모두 알기에 policy가 변경되더라도 stationary하다.
agent  i에서 얻은 obs-act pair를 Q-function에 넣어주면 Q(x,a1,a2,...aN)으로 표현한다.
참고로 x = (obs1,obs2,obs3,...obsN)이다.

critic network를 update하는데 필요한 loss식은 아래와 같다.

<img src = "/surabanke/assets/images/MADDPG_loss.png" width = "400">

μ'는 우리가 update해야하는 target policy이다. TD-target y는 target policy로 one-step(or n-step)앞선 (centralized)Q-value를 구한다.
(*MADDPG의 기반이 되는 DPG는 off policy이므로 behaviour policy와 target policy가 다르다.*)

actor network를 update하는데 필요한 policy gradient 식은 아래와 같다.

<img src = "/surabanke/assets/images/MADDPG.png" width = "400">

그러나 critic의 MSE loss식을 구하려면 다른 agent의 target policy를 알아야 y를 구할 수 있다.
다른 agents의 policy를 직접 알려줘도 되지만 여기서는 approximation policy를 사용한다.
(*논문에서 사용한 MADDPG policy는 하나에 64개 노드가 있는 ReLU-MLP layer 두개로 학습되었다.*)

#####다른 agent들의 policy 추론

다른 agent들의 policy를 명시해주는 가정을 없애려면 policy μj(agent j의 true target policy)를 근사해야 한다.
근사값 μj^는 (7)식을 maximize하는 방향으로 학습하며 구해진다.


<img src = "/surabanke/assets/images/MADDPG_approximate.png" width = "400">


log(agent j의 근사 policy) * λH 여기서 λH는 entropy regularizer라고 한다. H는 policy distribution의 정보량(entropy)이다.
위의 (6)번식이 진행되기 전에 (7)번식으로 policy들을 근사하고 y식이 계산한다.


<img src = "/surabanke/assets/images/approximate_TD_target.png" width = "400">

위의 식(8)에서 내 policy는 이미 알고 있으므로 참값을 넣어준다.


####Policy Ensembles

학습 중 policy가 바뀌면서 non-stationary한 환경의 문제를 해결할 수 있는 방법이 centralized training말고 더 있다.
policy ensemble은 policy를 agent마다 하나가 아니라 각기 다른 K개의 sub-policy를 두어 학습하는 방식이다.
한 episode마다 policy하나를 선택하여 학습한다. 그러므로 학습되었다고 행동이 달라지는 경우는 없다 다음 에피소드의 policy는 또다른 학습되지 않은 policy이다.
agent가 N개 agent 하나에 sub-policy k개 sub-policy 하나에 버퍼하나씩으로 replay buffer는 총 N*K개가 된다.
그리고 agent i의 sub-policy를 평균내어 policy gradient를 계산한다.


####Environments and comparison
다음으로 환경설명을 한다. cooperative한 환경도 있고 competitive한 환경 또는 에이전트들의 목표가 서로 상충되지만 모든 agent는 global한 return을 최대화 시켜야하는 mixed cooperative & competitive한 환경

여러가지 환경에서 MADDPG와 single DDPG를 비교한 그래프가 왼쪽, MADDPG에서 single policy와 ensemble policy로 각자 환경에서 학습한 결과그래프가 오른쪽에 있다.

<img src = "/surabanke/assets/images/MADDPG_comparison.png" width = "400">

가장 쉬운 환경인 cooperative communication(listener는 target landmark를 모름 speaker는 target을 알고 있고 현재 listener가 관찰하고 있는 landmark색을 알려 줄 수 있다. )으로 학습을 하는데도 traditional RL methods는 좋은 policy를 찾는데 실패했다고 한다.


<img src = "/surabanke/assets/images/MADDPG_comparison_1.png" width = "400">


Figure(7)그래프의 왼쪽은 다른 agent들 policy 근사해서 학습한 모델 average reward와 true policy를 알려주고 학습해서 나온 average reward의 비교 결과이고 오른쪽은 true policy와 approximation policy로 cooperative communication task를 했을때 각각의listener와 speaker agent들의 KL divergence이다. (스피커 에이전트는 리스너보다 더 단순한동작을 하기때문에 KL-divergence도 true policy와 별로 차이가 안나는것같다.)
